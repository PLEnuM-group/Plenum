{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run these every time there are updates to resolution, binning, or mephistograms\n",
    "# load and prepare the energy and angular resolutions\n",
    "%run ../../core/resolution.py\n",
    "# gather all ingredients and save them as unified 'mephistogram' data structure\n",
    "%run ../../core/prepare_histograms.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mkl\n",
    "mkl.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from tools import ang_dist\n",
    "from settings import *\n",
    "\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "from copy import copy\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from aeff_calculations import calc_aeff_factor\n",
    "from fluxes import (\n",
    "    astro_flux,\n",
    "    atmo_background,\n",
    "    cut_off,\n",
    "    power_law,\n",
    ")\n",
    "import mephisto\n",
    "from matplotlib.colors import LogNorm\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.stats import chi2, norm\n",
    "from tools import array_source_interp, poisson_llh\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10yr data release\n",
    "# we take only the full detector configurations >= IC86-II,\n",
    "# because these we also took for the resolutions and effective area\n",
    "exp_data = []\n",
    "for fn in glob(\"/data/plenum/icecube_10year_ps/events/IC86*.csv\"): #TODO copy files\n",
    "    if \"_I_\" in fn: continue\n",
    "    print(fn)\n",
    "    exp_data.extend(np.genfromtxt(fn, names=True))\n",
    "exp_data = np.array(exp_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angular_distances = np.rad2deg(\n",
    "    ang_dist(\n",
    "        ngc1068.ra.rad,\n",
    "        ngc1068.dec.rad,\n",
    "        np.deg2rad(exp_data[\"RAdeg\"]),\n",
    "        np.deg2rad(exp_data[\"Decdeg\"]),\n",
    "    )\n",
    ")[0]\n",
    "\n",
    "\n",
    "# select events within the analysis window\n",
    "ngc_dist_mask = angular_distances <= delta_psi_max\n",
    "ngc_events = exp_data[ngc_dist_mask]\n",
    "ngc_angular_distances = angular_distances[ngc_dist_mask]\n",
    "\n",
    "# select events in declination band\n",
    "dec_distances = np.rad2deg(\n",
    "    ang_dist(\n",
    "        0,\n",
    "        ngc1068.dec.rad,\n",
    "        np.zeros_like(exp_data[\"RAdeg\"]),\n",
    "        np.deg2rad(exp_data[\"Decdeg\"]),\n",
    "    )\n",
    ")[0]\n",
    "\n",
    "ngc_dec_mask = ~ngc_dist_mask & (dec_distances <= delta_psi_max)\n",
    "ngc_dec_events = exp_data[ngc_dec_mask]\n",
    "ngc_dec_distances = dec_distances[ngc_dec_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngc1068 data release\n",
    "# new_ev = pd.read_csv(\"/home/lisajsch/Downloads/20220913_Evidence_for_neutrino_emission_from_the_nearby_active_galaxy_NGC_1068_data/ps_data_release/resources/event_list.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(ngc_dec_events[\"log10EGeV\"], bins=logE_reco_bins[::2])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"log10EGeV\")\n",
    "\n",
    "plt.figure(figsize=(18, 1))\n",
    "_ = plt.hist2d(ngc_dec_events[\"RAdeg\"], ngc_dec_events[\"Decdeg\"], bins=(360, 10))\n",
    "plt.xlabel(\"RA/deg\")\n",
    "plt.ylabel(\"dec/deg\")\n",
    "plt.xlim(0, 180)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "_ = plt.hist2d(ngc_events[\"RAdeg\"], ngc_events[\"Decdeg\"], bins=15)\n",
    "plt.scatter(\n",
    "    ngc1068.ra.deg, ngc1068.dec.deg, color=\"tomato\", marker=\"+\", s=1E5, alpha=0.8\n",
    ")\n",
    "plt.xlabel(\"RA/deg\")\n",
    "plt.ylabel(\"dec/deg\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we use the effective area and resolution functions of only IC86. Further updates will see the \"per detector config\" PDFs. Much of the discrepancy is mitigated by using data itself to describe the background PDF of reconstructed energy.\n",
    "\n",
    "# Effective area\n",
    "\n",
    "We use here only upgoing events with dec>-5deg, since this is the threshold for a pure data set with few muons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(LOCALPATH, \"effective_area_MH_upgoing.pckl\"), \"rb\") as f:\n",
    "    aeff_2d = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atmospheric background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(LOCALPATH, \"atmospheric_background_MH.pckl\"), \"rb\") as f:\n",
    "    bckg_histo = pickle.load(f)\n",
    "\n",
    "# check if histos are matching\n",
    "print(bckg_histo.match(aeff_2d[\"IceCube\"], verbose=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy resolution function\n",
    "\n",
    "Plotting code can be found in `energy_resolution.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(LOCALPATH, \"GP_Eres_mephistograms.pckl\"), \"rb\") as f: #\n",
    "    all_eres = pickle.load(f)\n",
    "# select only horizontal resolution\n",
    "baseline_eres = all_eres['dec-0.0']\n",
    "baseline_eres.normalize(axis=1) # normalize per logE_true\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PsiÂ²-Energy resolution\n",
    "\n",
    "NOTE: psi changes per detector over the day. For the sake of (optimistic) simplicity, we choose the horizontal resolution for all detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angular resolution\n",
    "with open(join(LOCALPATH, f\"Psi2-{delta_psi_max}_res_mephistograms.pckl\"), \"rb\") as f:\n",
    "    all_psi = pickle.load(f)\n",
    "# select only horizontal resolution\n",
    "e_psi2_grid = all_psi[\"dec-0.0\"]\n",
    "e_psi2_grid.normalize()\n",
    "\n",
    "# uniform, normalized background grid\n",
    "bckg_psi2_grid = mephisto.like(e_psi2_grid, fill_value=1)\n",
    "bckg_psi2_grid.normalize()\n",
    "\n",
    "e_psi2_grid.plot(norm=LogNorm())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flux definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we base the flux models on named-tuples\n",
    "PL_flux = namedtuple(\"PL_flux\", \"norm gamma E0 shape\")\n",
    "PLcut_flux = namedtuple(\"PLcut_flux\", \"norm gamma e_cut E0 shape\")\n",
    "LogP_flux = namedtuple(\"LogP_flux\", \"norm alpha beta E0 shape\")\n",
    "\n",
    "flux_collection = {\n",
    "    \"powerlaw\": PL_flux,\n",
    "    \"powerlaw with cutoff\": PLcut_flux,\n",
    "    \"log-parabola\": LogP_flux,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGC1068 source parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngc flux error contour\n",
    "gamma_c_68, phi_c_68 = np.loadtxt(join(LOCALPATH, \"ngc_paper_68_contour.txt\"))\n",
    "phi_c_68 /= 1E11\n",
    "\n",
    "# 10yr PS paper (data release)\n",
    "gamma_c_68_prev, phi_c_68_prev = np.loadtxt(join(LOCALPATH, \"tenyr_paper_68_contour.txt\"))\n",
    "phi_c_68_prev *= 1E-3\n",
    "\n",
    "ngc_flux = PL_flux(PHI_NGC, GAMMA_NGC, E0_NGC, \"powerlaw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic source config without dec coordinate\n",
    "src_config = dict(\n",
    "    sindec_mids=sindec_mids,\n",
    "    livetime=LIVETIME,\n",
    "    ewidth=ewidth,\n",
    "    dpsi_max=0,\n",
    "    grid_2d=e_psi2_grid,\n",
    ")\n",
    "# generic bg config\n",
    "bg_config = copy(src_config)\n",
    "bg_config[\"dpsi_max\"] = delta_psi_max\n",
    "bg_config[\"grid_2d\"] = bckg_psi2_grid\n",
    "\n",
    "# ... with ngc declination\n",
    "ngc_src_config = copy(src_config)\n",
    "ngc_src_config[\"dec\"] = ngc1068.dec.rad\n",
    "ngc_bg_config = copy(bg_config)\n",
    "ngc_bg_config[\"dec\"] = ngc1068.dec.rad\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up LLH function\n",
    "$ \\mathcal{L}({\\rm data}~k~ |~{\\rm hypothesis}~\\mu)\n",
    "    = \\prod_{{\\rm bin\\,}ij}^{N_{\\rm bins}} \\frac{\\mu_{ij}^{k_{ij}}}{k_{ij}!}\\cdot\n",
    "    \\exp \\left( -\\mu_{ij} \\right)$\n",
    "\n",
    "\n",
    "Background hypothesis $H_0(\\mu = N_B)$ : only atmospheric neutrino flux\n",
    "\n",
    "Signal hypothesis $H_1(\\mu = \\{N_B, N_S, \\gamma\\})$: atmospheric neutrino flux + astrophysical neutrino flux\n",
    "\n",
    "Idea: data ($k$) are the perfect representation of our expectation; the hypothesis ($\\mu$) is the model with the free parameters we'd like to know\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps_llh_2d_composite(\n",
    "    x,\n",
    "    mu_b_base,\n",
    "    mu_s_base,\n",
    "    k_i,\n",
    "    e_0,\n",
    "    phi_0,\n",
    "    shape,\n",
    "    energy_resolution,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"fit parameters in x:\n",
    "    x[0]: background normalization scaling\n",
    "    x[1]: signal normalization scaling\n",
    "    x[2:]: other signal parameters\n",
    "        see 'astro_flux' for further shapes and parameters\n",
    "\n",
    "    mu_b_base: background baseline\n",
    "    mu_s_base: effective area factor for signal\n",
    "    k_i: observation/asimov data\n",
    "    e_0: normalization energy, default E0_NGC\n",
    "    phi_0: normalization flux, default PHI_NGC\n",
    "    shape: flux shape\n",
    "    \"\"\"\n",
    "    mu_b = mu_b_base * x[0]\n",
    "    mu_s = astro_flux(\n",
    "        mu_s_base,\n",
    "        emids,\n",
    "        energy_resolution,\n",
    "        x[1],\n",
    "        flux_collection[shape](\n",
    "            phi_0, *x[2:], e_0, shape\n",
    "        ),  # here we generate a flux tuple with the current parameters\n",
    "    )\n",
    "    if verbose:\n",
    "        print(x[0], x[1], *x[2:])\n",
    "        print(flux_collection[shape](phi_0, *x[2:], e_0, shape))\n",
    "        print(np.sum(mu_b), np.sum(mu_s))\n",
    "    mu_i = mu_s + mu_b\n",
    "\n",
    "    return poisson_llh(mu_i, k_i)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Asimov data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idents = [\"IceCube\", \"Plenum-1\", \"Plenum-2\"]\n",
    "for ident in idents:\n",
    "    print(ident)\n",
    "    print(\"=\" * 10)\n",
    "\n",
    "    aeff_factor_bckg = calc_aeff_factor(aeff_2d[ident], **ngc_bg_config)\n",
    "    aeff_factor_signal = calc_aeff_factor(aeff_2d[ident], **ngc_src_config)\n",
    "    if \"Plenum\" in ident or \"P-ONE\" in ident:\n",
    "        aeff_factor_bckg += calc_aeff_factor(aeff_2d[\"IceCube\"], **ngc_bg_config)\n",
    "        aeff_factor_signal += calc_aeff_factor(aeff_2d[\"IceCube\"], **ngc_src_config)\n",
    "    # asimov data\n",
    "    k_b = atmo_background(\n",
    "        aeff_factor=aeff_factor_bckg,\n",
    "        bckg_vals=array_source_interp(ngc_src_config[\"dec\"], bckg_histo, sindec_mids, axis=1),\n",
    "        energy_resolution=baseline_eres,\n",
    "    )\n",
    "    # background estimation from data\n",
    "    bg_data = mephisto.like(k_b, fill_value=0)\n",
    "\n",
    "    # power law\n",
    "    k_s = astro_flux(\n",
    "        aeff_factor=aeff_factor_signal,\n",
    "        emids=emids,\n",
    "        energy_resolution=baseline_eres,\n",
    "        phi_scaling=1,\n",
    "        flux_shape=ngc_flux,  # powerlaw\n",
    "    )\n",
    "    k_i = k_s + k_b\n",
    "\n",
    "    print(\"Asimov data sum:\")\n",
    "    print(\"Background:\", np.sum(k_b))\n",
    "    print(\"Signal:\", np.sum(k_s))\n",
    "\n",
    "\n",
    "    dct = dict(\n",
    "        shape=\"powerlaw\",\n",
    "        mu_b_base=k_b,\n",
    "        mu_s_base=aeff_factor_signal,\n",
    "        k_i=k_i,\n",
    "        e_0=ngc_flux.E0,\n",
    "        phi_0=ngc_flux.norm,\n",
    "        energy_resolution=baseline_eres,\n",
    "    )\n",
    "\n",
    "    # fix signal normalization to 0\n",
    "    local_llh = lambda x: ps_llh_2d_composite((x[0], 0, 3), **dct)\n",
    "    global_llh = lambda x: ps_llh_2d_composite(x, **dct)\n",
    "\n",
    "    out_bckg = fmin_l_bfgs_b(\n",
    "        local_llh,\n",
    "        x0=(1,),\n",
    "        approx_grad=True,\n",
    "    )\n",
    "    print(\"Atmo-only normalization:\", out_bckg[0])\n",
    "\n",
    "    # this fit is not really needed, but good for crosschecking the code\n",
    "    out_comp = fmin_l_bfgs_b(\n",
    "        global_llh,\n",
    "        x0=(1, 1, ngc_flux.gamma),\n",
    "        approx_grad=True,\n",
    "    )\n",
    "    print(\"Signal fit parameters:\", out_comp[0])\n",
    "    print(\"TS check:\", global_llh((1, 1, ngc_flux.gamma)), \"(fit: \", out_comp[1], \")\")\n",
    "    print(\"-2 Delta LLH:\", out_bckg[1] - out_comp[1])\n",
    "\n",
    "    pval = chi2.sf(out_bckg[1] - out_comp[1], 2)\n",
    "    significance = norm.isf(pval)\n",
    "    print(\"P-VALUE:\", pval, \"SIGNIFICANCE:\", significance)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# real data\n",
    "## plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hist = mephisto.Mephistogram(\n",
    "    (ngc_angular_distances**2, ngc_events[\"log10EGeV\"]),\n",
    "    bins=k_s.bins,\n",
    "    axis_names=k_s.axis_names,\n",
    "    make_hist=True\n",
    ")\n",
    "\n",
    "data_hist.plot()\n",
    "plt.ylim(2, 5)\n",
    "\n",
    "data_hist.sum(axis=1, return_mephisto=True).plot()\n",
    "plt.gca().set_xlim(0, 4)\n",
    "data_hist.sum(axis=0, return_mephisto=True).plot()\n",
    "\n",
    "plt.xlim(2, 5)\n",
    "plt.gca().set_yscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the background PDF from data\n",
    "# use the energy distribution for one axis...\n",
    "data_bg_hist, _ = np.histogram(ngc_dec_events[\"log10EGeV\"], bins=logE_reco_bins)\n",
    "\n",
    "# ... and a uniform distribution for the angular distance axis\n",
    "data_bg = mephisto.Mephistogram(\n",
    "    np.tile(data_bg_hist, len(psi2_mids)).reshape(data_hist.shape),\n",
    "    bins=data_hist.bins,\n",
    "    axis_names=data_hist.axis_names,\n",
    "    make_hist=False,\n",
    ")\n",
    "# normalize such that it matches the data\n",
    "# then the BG normalization will be close to 1 in the fit\n",
    "data_bg.normalize(mode=\"full\")\n",
    "data_bg *= data_hist.sum()\n",
    "data_bg.plot()\n",
    "plt.ylim(2, 5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_from_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ident = \"IceCube\"\n",
    "aeff_factor_bckg = calc_aeff_factor(aeff_2d[ident], **ngc_bg_config)\n",
    "aeff_factor_signal = calc_aeff_factor(aeff_2d[ident], **ngc_src_config)\n",
    "# asimov data with baseline fluxes\n",
    "\n",
    "### we use the bg from data instead! ###\n",
    "if bg_from_data:\n",
    "    k_b = data_bg\n",
    "else:\n",
    "    k_b = atmo_background(\n",
    "    aeff_factor=aeff_factor_bckg,\n",
    "    bckg_vals=array_source_interp(\n",
    "        ngc_src_config[\"dec\"], bckg_histo, sindec_mids, axis=1\n",
    "    ),\n",
    "    energy_resolution=baseline_eres,\n",
    "    )\n",
    "\n",
    "dct = dict(\n",
    "    shape=\"powerlaw\",\n",
    "    mu_b_base=k_b,  # we use the bg from data instead!\n",
    "    mu_s_base=aeff_factor_signal,\n",
    "    k_i=data_hist,\n",
    "    e_0=ngc_flux.E0,\n",
    "    phi_0=ngc_flux.norm,\n",
    "    energy_resolution=baseline_eres,\n",
    ")\n",
    "\n",
    "bg_llh = lambda x: ps_llh_2d_composite((x[0], 0, 3), **dct)\n",
    "\n",
    "out_bckg = fmin_l_bfgs_b(\n",
    "    bg_llh,\n",
    "    x0=(1.1,),\n",
    "    approx_grad=True,\n",
    ")\n",
    "print(\"Atmo-only normalization:\", out_bckg[0])\n",
    "\n",
    "# make parameter scans as crosscheck\n",
    "# background-only scan\n",
    "bg_ts = []\n",
    "for nb in np.linspace(0.5, 1.5, 100):\n",
    "    bg_ts.append({\"ts\": ps_llh_2d_composite((nb, 0, 3), **dct) - out_bckg[1], \"nb\": nb})\n",
    "\n",
    "bg_ts = pd.DataFrame(bg_ts)\n",
    "bg_ts.plot(x=\"nb\", y=\"ts\")\n",
    "plt.axvline(out_bckg[0])\n",
    "\n",
    "# full optimization of all parameters\n",
    "\n",
    "global_llh = lambda x: ps_llh_2d_composite(x, **dct)\n",
    "\n",
    "# S+B fit\n",
    "out_global = fmin_l_bfgs_b(\n",
    "    global_llh,\n",
    "    x0=(1, 1, ngc_flux.gamma),\n",
    "    bounds=[(1e-3, 10), (1e-3, 10), (1e-3, 10)],\n",
    "    approx_grad=True,\n",
    ")\n",
    "print(\"Signal fit parameters:\", out_global[0])\n",
    "\n",
    "# significance\n",
    "pval = chi2.sf(out_bckg[1] - out_global[1], 2)\n",
    "significance = norm.isf(pval)\n",
    "print(\"P-VALUE:\", pval, \"SIGNIFICANCE:\", significance)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data PDF agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare psi and energy histograms of data and best-fit\n",
    "ident = \"IceCube\"\n",
    "aeff_factor_bckg = calc_aeff_factor(aeff_2d[ident], **ngc_bg_config)\n",
    "aeff_factor_signal = calc_aeff_factor(aeff_2d[ident], **ngc_src_config)\n",
    "\n",
    "# asimov data with baseline fluxes\n",
    "if bg_from_data:\n",
    "    k_b = data_bg * out_global[0][0]\n",
    "else:\n",
    "    k_b = atmo_background(\n",
    "        aeff_factor=aeff_factor_bckg * out_global[0][0],\n",
    "        bckg_vals=array_source_interp(\n",
    "            ngc_src_config[\"dec\"], bckg_histo, sindec_mids, axis=1\n",
    "        ),\n",
    "        energy_resolution=baseline_eres,\n",
    "    )\n",
    "\n",
    "bf_ngc_flux = PL_flux(PHI_NGC * out_global[0][1], out_global[0][2], E0_NGC, \"powerlaw\")\n",
    "k_s = astro_flux(\n",
    "    aeff_factor=aeff_factor_signal,\n",
    "    emids=emids,\n",
    "    energy_resolution=baseline_eres,\n",
    "    phi_scaling=1,\n",
    "    flux_shape=bf_ngc_flux,  # powerlaw\n",
    ")\n",
    "\n",
    "k_sum_bf = k_s + k_b\n",
    "\n",
    "# plot data and baseline flux comparison\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(\n",
    "    k_b.bin_mids[0],\n",
    "    k_b.sum(axis=1),\n",
    "    ds=\"steps-mid\",\n",
    "    label=\"BF background\",\n",
    "    color=\"gray\",\n",
    ")\n",
    "\n",
    "p = plt.plot(\n",
    "    k_s.bin_mids[0], k_s.sum(axis=1), ds=\"steps-mid\", label=\" BF signal\", color=\"gold\"\n",
    ")\n",
    "\n",
    "p = plt.plot(\n",
    "    k_sum_bf.bin_mids[0],\n",
    "    k_sum_bf.sum(axis=1),\n",
    "    ds=\"steps-mid\",\n",
    "    label=\"BF background + signal\",\n",
    ")\n",
    "plt.plot(\n",
    "    data_hist.bin_mids[0],\n",
    "    data_hist.sum(axis=1),\n",
    "    ds=\"steps-mid\",\n",
    "    label=\"data\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "plt.xlim(0, 2)\n",
    "plt.legend(loc=0, fontsize=\"small\")\n",
    "\n",
    "# plot data and baseline flux comparison\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    k_b.bin_mids[1],\n",
    "    k_b.sum(axis=0),\n",
    "    ds=\"steps-mid\",\n",
    "    label=\"BF background\",\n",
    "    color=\"gray\",\n",
    ")\n",
    "\n",
    "p = plt.plot(\n",
    "    k_s.bin_mids[1], k_s.sum(axis=0), ds=\"steps-mid\", label=\" BF signal\", color=\"gold\"\n",
    ")\n",
    "\n",
    "p = plt.plot(\n",
    "    k_sum_bf.bin_mids[1],\n",
    "    k_sum_bf.sum(axis=0),\n",
    "    ds=\"steps-mid\",\n",
    "    label=\"BF background + signal\",\n",
    ")\n",
    "plt.plot(\n",
    "    data_hist.bin_mids[1],\n",
    "    data_hist.sum(axis=0),\n",
    "    ds=\"steps-mid\",\n",
    "    label=\"data\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "plt.xlim(2, 6)\n",
    "plt.legend(loc=0, fontsize=\"small\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-1, 3e2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## profile likelihood \n",
    "Contour -> optimize nb on a gamma-ns grid\n",
    "Also, cross-check the BF results for the Global LLH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_ts = []\n",
    "num = 30\n",
    "g_space = np.linspace(2.45, 4.2, num=num+1)\n",
    "ns_space = np.logspace(-0.9, 0.2, num=num)\n",
    "\n",
    "for g, s in product(g_space, ns_space):\n",
    "    tmp = {}\n",
    "    tmp[\"ns\"] = s\n",
    "    tmp[\"gamma\"] = g\n",
    "\n",
    "    _llh = lambda x: ps_llh_2d_composite((x[0], s, g), **dct)\n",
    "\n",
    "    # S+B fit\n",
    "    out_profile = fmin_l_bfgs_b(\n",
    "        _llh,\n",
    "        x0=(1, ),\n",
    "        bounds=[(1E-3, 10),],\n",
    "        approx_grad=True,\n",
    "    )\n",
    "\n",
    "    tmp[\"ts\"] = out_profile[1]\n",
    "    tmp[\"delta_ts\"] = out_profile[1] - out_global[1]\n",
    "    tmp[\"nb\"] = out_profile[0][0]\n",
    "    sig_ts.append(tmp)\n",
    "\n",
    "sig_ts = pd.DataFrame(sig_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_ts[\"norm\"] = sig_ts[\"ns\"] * ngc_flux.norm\n",
    "sig_ts_piv = sig_ts.pivot(index=\"norm\", columns=\"gamma\", values=\"delta_ts\")\n",
    "cvals = [0.32, 0.05]\n",
    "lbs = [\"68%\", \"95%\"]\n",
    "levels = chi2.isf(cvals, 2)\n",
    "cur_cmap = sns.cubehelix_palette(\n",
    "    start=0.75, rot=-0.5, light=1, dark=0.5, as_cmap=True, reverse=True\n",
    ")\n",
    "plt.pcolormesh(\n",
    "    sig_ts_piv.columns, sig_ts_piv.index, sig_ts_piv, vmin=0, vmax=10, cmap=cur_cmap\n",
    ")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.plot(gamma_c_68_prev, phi_c_68_prev, color=\"k\", ls=\":\")\n",
    "plt.plot(3.2, 3e-14, color=\"k\", marker=\"o\", label=\"10yr paper\", ms=12, ls=\":\")\n",
    "\n",
    "plt.plot(gamma_c_68, phi_c_68 * 1E-3, color=\"k\", ls=\"--\")\n",
    "plt.plot(3.2, 5e-14, color=\"k\", marker=\"s\", label=\"NGC paper\", ms=12, ls=\"--\")\n",
    "\n",
    "\n",
    "plt.plot(\n",
    "    out_global[0][2],\n",
    "    out_global[0][1] * ngc_flux.norm,\n",
    "    color=\"tomato\",\n",
    "    marker=\"*\",\n",
    "    ms=12,\n",
    "    label=\"PLEnuM BF\",\n",
    ")\n",
    "sc = plt.contour(\n",
    "    sig_ts_piv.columns,\n",
    "    sig_ts_piv.index,\n",
    "    sig_ts_piv,\n",
    "    levels=levels,\n",
    "    lw=3,\n",
    "    colors=[\"tomato\", \"darkred\"],\n",
    ")\n",
    "\n",
    "fmt = {l: s for l, s in zip(sc.levels, lbs)}\n",
    "plt.clabel(sc, levels, fmt=fmt, fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(r\"$\\Phi_0 / {\\rm GeV\\, cm^2\\, s}$\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(loc=0)\n",
    "if bg_from_data:\n",
    "    title_str = \"Background from data\"\n",
    "else:\n",
    "    title_str = \"Background from MCEq\"\n",
    "plt.title(title_str)\n",
    "plt.ylim(5E-15, 1E-13)\n",
    "plt.xlim(2.4, 4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLEnuM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ba996df2b34c8b838a6723f471b00873bd81d83dd405585774c96822ec1df43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
